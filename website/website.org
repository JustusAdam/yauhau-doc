* The Problem
** Microservices:
   The software architecture of most large internet services, if not all, consists of microservices. Microservices are fine-grained components of the system, each with a particular functionality, and which communicate with lightweight protocols.
   Systems nowadays consist of several such microservices, interconnected by large hierarchies of dependencies.
   In order to program software in these microservice-oriented architectures, software engineers need to use complex libraries to manage the interaction. If performance and latency are vital, as is in many cases, programmers must also implement efficient
   concurrent system calls, and leverage difficult-to use synchronization methods, like locks or futures.
** Handling I/O:
   A particularly important task performed by many microservices are I/O operations, e.g., to a database or other stored data on a disk. Compared to much of the usual computation in these systems, the latency incurred through I/O usually dominates the latency of the whole system.
   For this reason, it is particularly important to optimize I/O as much as possible. Many I/O-based services allow batched calls, e.g. to retrieve an array of data points from a database, instead of a single one. When several I/O calls to different places are issued,
   it is crucial to know how they depend on each other. Independent calls can be executed concurrently, significantly reducing latency, whereas dependent ones have to be executed in sequence if the service is to operate correctly.
** Getting this right: very complex code
   Manage the different microservice protocols, while writing code that execute correctly, and efficiently, is a dauting task. In practice, it usually forces a trade-off between readability, mantainability and efficiency:
   To improve efficiency, the developer has to explicitly manage I/O calls and concurrency, at the cost of code readability and maintainability.
   On the one hand, blocking (synchronous) I/O calls produce the most readable and maintainable code but result in a sequential execution of all requests.
   On the other hand, non-blocking (asynchronous) I/O calls execute remote services in parallel but require the use of concurrency constructs such as threads or events.
   Threads use locks which can introduce deadlocks, while events clu er the code signi cantly.  us, both approaches add additional complexity and result in code that is much less clean and concise.

* Ÿauhau
  To overcome these problems, we present Ÿauhau. It allows enginieers to write simple code that is mantainable and concise, without sacrificing the efficiency of batching and concurrent I/O calls.  
** Simple programming
   Ÿauhau is an extension to the Ohua framework. Ohua works on Closure, a dialect of Lisp for the JVM. Programs written with Ÿauhau are simple because of the declarative, functional nature of LISP.
   A programmer does not need to think about what is executed when, nor label dependencies or introduce complex constructs for concurrency and parallelism.
   Instead, programmers using Ÿauhau only need to write their alorithm in a simple, declarative style and the complier takes care of squeezing out efficiency when executing.
** Efficient execution
   When the Ohua compiler reads an Ohua program, it uses analysis methods to understand what can be executed concurrently, and what cannot. It does so by leveraging the declarative nature of the programs, without any explicit imput from the programmer.
   Ohua automatically executes applications using all the concurrency and parallelism extracted, managing synchronization itself. On top of Ohua, the Ÿauhau extensions understand which calls perform I/O and automatically batch all I/O calls to the same source, if
   it allows batching. In this way, Ÿauhau allows programmers to write simple code which automatically works with close to maximal I/O efficiency.

** Example!
   
* How does it work?
** Ohua: Implicit concurrency and parallelism through dataflow 
   Ohua compiles a Clojure-style program into a so-called dataflow graph. Such a graph represents the different functions and algorithms in the computation and the dependencies between the produced data. (Example).
   
** Ÿauhau: Dataflow graph rewrites 
   In order to execute I/O calls efficiently, Ÿauhau uses a series of rewrites to the dataflow graph of Ohua, that allow it to batch I/O calls to the same source whenever possible, while mantaining the functionality of the program. 
   
** Example!
   
* Some benchmarks
  
Similar to Ÿauhau, there are other frameworks which attempt to batch calls together and minimize the cost of I/O. Haxl, by Facebook, does so using a concept called "Applicative Functors" in Haskell. Muse is a similar library, based on Haxl, for Clojure.
Finally, Stitch, by Twitter, also provides a similar functionality to Haxl, Muse and Ÿauhau. Since Stitch is closed-source, we compare Ÿauhau only to Haxl and Muse.

** Baseline Comparison
We compare Ÿauhau to Haxl, Muse, and for reference, to a sequential execution. We do this using randomly generated microservice-based applications, with so-called "level graphs". The number of levels of a graph represent the total complexity the application. The more levels, the larger and more complex the application.
Ÿauhau conistently performs better than all other systems, or at least as good in all cases.

** Code Style
Haxl and Muse allow for different styles of coding, an "Applicative" style (named after Applicative Functions, mentioned above), or a "Monadic" style. The latter is simpler to write, but as can be seen in the graph, results in worst performance. Except for Ÿauhau,
which achieves the best performance of all systems, independent of the code style.

** I/O imbalance
Not all sources of I/O are equal. When one microservice requires too long to execute in comparison to the rest, most systems' performance will suffer an additional penalty.
This happens because these systems execute I/O calls in rounds, and block the execution until all I/O calls in a round have been executed. Not Ÿauhau. The dataflow execution model
allows a Ÿauhau program to continue executing everything that can be executed while waiting for a particularly laggy I/O call to finish.

** Modular designs
Mantainable and debuggable software has to be written in a modular fashion. This is usually done by writing functions, grouping them into libraries, and reusing the functionality. 
However, in the other systems, this leads to worse execution behavior. Haxl struggles to understand some dependencies that go beyond the borders of a function, and Muse doesn't do it at all.
Ÿauhau with its dataflow model, on the other hand, can extract the dependencies with surgical precision. To measure this, instead of making the application more complex, we took a single large application and added more calls to other functions in the random graphs, 
with different probabilites. The result is an application that has more calls to other functions in its body. We see that for Ÿauhau, this does not change the number of calls significantly (it changes at all because of the random nature of the experiment), whereas
Haxl and Muse struggle with more function calls.




